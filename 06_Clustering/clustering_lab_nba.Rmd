---
title: "Clustering Lab"
author: "Po Wei Tsao"
date: "10/08/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Goal: Know how to make decisions and answer questions using clustering. 

# Repeat the clustering process only using the Rep house votes dataset
# - What differences and similarities did you see between how the clustering 
# worked for the datasets?

```{r}
library(tidyverse)
library(plotly)
library(htmltools)
library(devtools)
library(caret)
library(NbClust)
```

```{r}
#Load the data
nba_data = read.csv("../data/nba2020-21.csv")
View(nba_data)

nba_salaries = read.csv("../data/nba_salaries_21.csv")
View(nba_salaries)
```

```{r}
#Select the variables to be included in the cluster 
cols = c("Player", "FG.", "eFG.", "FT.", "TRB", "AST", "STL", "BLK", "TOV", "PTS")
clust_data_nba = nba_data[, cols]
clust_data_nba = clust_data_nba[complete.cases(clust_data_nba),]

# View(clust_data_nba)

standardize = function(data_in){
  #standardizing numeric data
  (new_data <- scale(data_in, center = TRUE, scale = TRUE))
  new_data

}
numeric_cols = names(select_if(clust_data_nba, is.numeric))
clust_data_nba[numeric_cols] <- as_tibble(lapply(clust_data_nba[numeric_cols], standardize))
# standardized_nba_clust_data = do.call(rbind.data.frame, lapply(clust_data_nba, standardize))
View(clust_data_nba)

```


```{r}
#Evaluate several different number of clusters
explained_variance = function(data_in, k){
  
  # Running the kmeans algorithm.
  set.seed(1)
  kmeans_obj = kmeans(data_in, centers = k, algorithm = "Lloyd", iter.max = 30)
  
  # Variance accounted for by clusters:
  # var_exp = intercluster variance / total variance
  var_exp = kmeans_obj$betweenss / kmeans_obj$totss
  var_exp  
}

player_names = clust_data_nba[0:1]
str(player_names)

drops = c("Player")
clust_data_nba = clust_data_nba[ , !(names(clust_data_nba) %in% drops)]

# View(clust_data_nba)

#Apply my function to the dataframe we have
explained_var_nba = sapply(1:10, explained_variance, data_in = clust_data_nba)

# View(explained_var_nba)


# Data for ggplot2.
# elbow_data_nba = data.frame(k = 1:10, explained_var_nba)
# View(explained_var_nba)
elbow_data_nba = data.frame(k = 1:10, explained_var_nba)
View(elbow_data_nba)
```

```{r}
#Create a elbow chart of the output 
# View(explained_var_nba)
# View(explained_var_Dem)
# 
# View(elbow_data_Dem)
# View(elbow_data_nba)

View(elbow_data_nba)
ggplot(elbow_data_nba, 
       aes(x = k,  
           y = explained_var_nba)) + 
  geom_point(size = 4) +           #<- sets the size of the data points
  geom_line(size = 1) +            #<- sets the thickness of the line
  xlab('k') + 
  ylab('Inter-cluster Variance / Total Variance') + 
  theme_light()

# With the elbow method, it looks like 2 clusters is a good choice
```

```{r}
#Use NbClust to select a number of clusters
library(NbClust)

# Run NbClust.
(nbclust_obj_nba = NbClust(data = clust_data_nba, method = "kmeans"))

# View the output of NbClust.
nbclust_obj_Dem

# View the output that shows the number of clusters each method recommends.
View(nbclust_obj_Dem$Best.nc)
```

```{r}
#Display the results visually
freq_k_Dem = nbclust_obj_Dem$Best.nc[1,]
freq_k_Dem = data.frame(freq_k_Dem)
View(freq_k_Dem)

# Check the maximum number of clusters suggested.
max(freq_k_Dem)

#essentially resets the plot viewer back to default
dev.off()

# Plot as a histogram.
ggplot(freq_k_Dem,
       aes(x = freq_k_Dem)) +
  geom_bar() +
  scale_x_continuous(breaks = seq(0, 15, by = 1)) +
  scale_y_continuous(breaks = seq(0, 12, by = 1)) +
  labs(x = "Number of Clusters",
       y = "Number of Votes",
       title = "Cluster Analysis")
```

```{r}
#Using the recommended number of cluster compare the quality of the model 
#with 2 clusters 
```


```{r}
#Bonus: Create a 3d version of the output
```

```{r}
#! also do practice for the rep/dem data set and submit it.
```

In a separate Rmarkdown document work through a similar process 
with the NBA data (nba2020-21 and nba_salaries_21), merge them together. 

You are a scout for the worst team in the NBA, probably the Wizards. Your 
general manager just heard about Data Science and thinks it can solve all the
teams problems!!! She wants you to figure out a way to find players that are 
high performing but maybe not highly paid that you can steal to get the team 
to the playoffs! 

Details: 

- Determine a way to use clustering to estimate based on performance if 
players are under or over paid, generally. 
- Then select three players you believe would be best your team and explain why. 
- Provide a well commented and clean (knitted) report of your findings that can 
be presented to your GM. Include a rationale for variable selection, details 
on your approach and a overview of the results with supporting visualizations. 
- Create a new repo for this assignment either for yourself and 
submit the link along with your knitted report. 

Hints:

- Salary is the variable you are trying to understand 
- You can include numerous performance variables in the clustering but when 
interpreting you might want to use graphs that include variables that are the 
most correlated with Salary
- You'll need to standardize the variables before performing the clustering
- Be specific about why you selected the players that you did, more detail is 
better
- Use good coding practices, comment heavily, indent, don't use for loops unless
totally necessary and create modular sections that align with some outcome. If 
necessary create more than one script,list/load libraries at the top and don't 
include libraries that aren't used. 
  





