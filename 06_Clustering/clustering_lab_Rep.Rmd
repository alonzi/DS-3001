---
title: "Clustering Lab"
author: "Po Wei Tsao"
date: "10/08/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Goal: Know how to make decisions and answer questions using clustering. 

# Repeat the clustering process only using the Rep house votes dataset
# - What differences and similarities did you see between how the clustering 
# worked for the datasets?

```{r}
library(tidyverse)
library(plotly)
library(htmltools)
library(devtools)
library(caret)
library(NbClust)
```

```{r}
#Load the data
house_votes_Rep = read.csv("../data/house_votes_Rep.csv")

table(house_votes_Rep$party.labels)
View(house_votes_Rep)
```

```{r}
#Select the variables to be included in the cluster 
View(house_votes_Rep)
clust_data_Rep = house_votes_Rep[, c("aye", "nay", "other")]
View(clust_data_Rep)
```

```{r}
#Run the clustering algo with 2 centers
set.seed(1)
kmeans_obj_Rep = kmeans(clust_data_Rep, centers = 2, 
                        algorithm = "Lloyd")
kmeans_obj_Rep
```

```{r}
#View the results
head(kmeans_obj_Rep)
```

```{r}
#Visualize the output
party_clusters_Rep = as.factor(kmeans_obj_Rep$cluster)
View(party_clusters_Rep)

View(house_votes_Rep)
View(party_clusters_Rep)

ggplot(house_votes_Rep, aes(x = aye, 
                            y = nay,
                            shape = party_clusters_Rep)) + 
  geom_point(size = 6) +
  ggtitle("Aye vs. Nay votes for Republican-introduced bills") +
  xlab("Number of Aye Votes") +
  ylab("Number of Nay Votes") +
  scale_shape_manual(name = "Cluster", 
                     labels = c("Cluster 1", "Cluster 2"),
                     values = c("1", "2")) +
  theme_light()
```

```{r}
#Evaluate the quality of the clustering 
ggplot(house_votes_Rep, aes(x = aye, 
                            y = nay,
                            color = party.labels,  #<- tell R how to color 
                            #   the data points
                            shape = party_clusters_Rep)) + 
  geom_point(size = 6) +
  ggtitle("Aye vs. Nay votes for Republican-introduced bills") +
  xlab("Number of Aye Votes") +
  ylab("Number of Nay Votes") +
  scale_shape_manual(name = "Cluster", 
                     labels = c("Cluster 1", "Cluster 2"),
                     values = c("1", "2")) +
  scale_color_manual(name = "Party",         #<- tell R which colors to use and
                     #   which labels to include in the legend
                     labels = c("Democratic", "Republican"),
                     values = c("blue", "red")) +
  theme_light()


# Save your graph. For Windows, use setwd("C:/file path")

ggsave("US House Votes for Rep Bills.png", 
       width = 10, 
       height = 5.62, 
       units = "in")
```

```{r}
#Use the function we created to evaluate several different number of clusters

explained_variance = function(data_in, k){
  
  # Running the kmeans algorithm.
  set.seed(1)
  kmeans_obj = kmeans(data_in, centers = k, algorithm = "Lloyd", iter.max = 30)
  
  # Variance accounted for by clusters:
  # var_exp = intercluster variance / total variance
  var_exp = kmeans_obj$betweenss / kmeans_obj$totss
  var_exp  
}

View(clust_data_Rep)

# The sapply() function plugs in several values into our explained_variance function.
#sapply() takes a vector, lapply() takes a dataframe
explained_var_Rep = sapply(1:10, explained_variance, data_in = clust_data_Rep)

View(explained_var_Rep)


# Data for ggplot2.
elbow_data_Rep = data.frame(k = 1:10, explained_var_Rep)
View(elbow_data_Rep)

```

```{r}
#Create a elbow chart of the output 

ggplot(elbow_data_Rep, 
       aes(x = k,  
           y = explained_var_Rep)) + 
  geom_point(size = 4) +           #<- sets the size of the data points
  geom_line(size = 1) +            #<- sets the thickness of the line
  xlab('k') + 
  ylab('Inter-cluster Variance / Total Variance') + 
  theme_light()
```

```{r}
#Use NbClust to select a number of clusters

library(NbClust)

# Run NbClust.
(nbclust_obj_Rep = NbClust(data = clust_data_Rep, method = "kmeans"))

# View the output of NbClust.
nbclust_obj_Rep

# View the output that shows the number of clusters each method recommends.
View(nbclust_obj_Rep$Best.nc)

# Subset the 1st row from Best.nc and convert it 
# to a data frame so ggplot2 can plot it.
freq_k_Rep = nbclust_obj_Rep$Best.nc[1,]
freq_k_Rep = data.frame(freq_k_Rep)
View(freq_k_Rep)

# Check the maximum number of clusters suggested.
max(freq_k_Rep)

#essentially resets the plot viewer back to default
dev.off()


```

I don't notice a really big difference between NB clust and the elbow method for this data set. Both seem to point to 2 clusters being the right choice here.

```{r}
#Display the results visually 
# Plot as a histogram.
ggplot(freq_k_Rep,
       aes(x = freq_k_Rep)) +
  geom_bar() +
  scale_x_continuous(breaks = seq(0, 15, by = 1)) +
  scale_y_continuous(breaks = seq(0, 12, by = 1)) +
  labs(x = "Number of Clusters",
       y = "Number of Votes",
       title = "Cluster Analysis")
```

```{r}
#Using the recommended number of cluster compare the quality of the model 
#with 2 clusters 
set.seed(1)
kmeans_obj_Rep = kmeans(clust_data_Rep, centers = 3, algorithm = "Lloyd")

# this is the output of the model. 
kmeans_obj_Rep$cluster

house_votes_Rep_3cluster = house_votes_Rep

house_votes_Rep_3cluster$clusters <- kmeans_obj_Rep$cluster
View(house_votes_Rep_3cluster)

# drop the name variable, won't be helpful
tree_data <- house_votes_Rep_3cluster[,-1]
str(tree_data)
# change 1 and 5 to factors
tree_data[,c(1,5)] <- lapply(tree_data[,c(1,5)], as.factor)
# do we need to normalize? 
      #no


# Split 
train_index <- createDataPartition(tree_data$party.labels,
                                           p = .7,
                                           list = FALSE,
                                           times = 1)
train <- tree_data[train_index,]
tune_and_test <- tree_data[-train_index, ]

#The we need to use the function again to create the tuning set 

tune_and_test_index <- createDataPartition(tune_and_test$party.labels,
                                           p = .5,
                                           list = FALSE,
                                           times = 1)

tune <- tune_and_test[tune_and_test_index, ]
test <- tune_and_test[-tune_and_test_index, ]

dim(tune)
dim(test)

# Create our features and target for training of the model. 

features <- as.data.frame(train[,-1])
target <- train$party.labels


set.seed(1980)
party_dt <- train(x=features,
                    y=target,
                    method="rpart")

# This is more or less a easy target but the clusters are very predictive. 
party_dt
varImp(party_dt)

# Let's predict and see how we did. 
#! gives us the accurancy of the features.
dt_predict_1 = predict(party_dt,tune,type= "raw")

confusionMatrix(as.factor(dt_predict_1), 
                as.factor(tune$party.labels), 
                dnn=c("Prediction", "Actual"), 
                mode = "sens_spec")



```


```{r}
View(house_votes_Rep)

# Assign colors by party in a new data frame.
party_color3D_Rep = data.frame(party.labels = c("Democrat", "Republican"),
                               color = c("blue", "red"))

View(party_color3D_Rep)

View(party_clusters_Rep)

# Join the new data frame to our house_votes_Dem data set.
house_votes_color_Rep = inner_join(house_votes_Rep, party_color3D_Rep)

house_votes_color_Rep$clusters <- (party_clusters_Rep)

str(house_votes_color_Rep)

#! replacing special characters in last names
house_votes_color_Rep$Last.Name <- gsub("[^[:alnum:]]", "", house_votes_color_Rep$Last.Name)

# Use plotly to do a 3d imaging 

fig <- plot_ly(house_votes_color_Rep, 
               type = "scatter3d",
               mode="markers",
               symbol = ~clusters,
               x = ~aye, 
               y = ~nay, 
               z = ~other,
               color = ~color,
               colors = c('#0C4B8E','#BF382A'), 
               text = ~paste('Representative:',Last.Name,
                             "Party:",party.labels))


fig
```





